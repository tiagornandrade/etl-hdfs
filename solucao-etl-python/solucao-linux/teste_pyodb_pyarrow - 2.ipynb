{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.6.6",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "code",
            "source": [
                "import pandas as pd\r\n",
                "import pyodbc\r\n",
                "import pyarrow as pa\r\n",
                "import pyarrow.parquet as pq\r\n",
                "import os\r\n",
                "import subprocess\r\n",
                "import shutil\r\n",
                "import glob\r\n",
                "from datetime import date, datetime\r\n",
                "from time import time\r\n",
                "from subprocess import Popen, PIPE"
            ],
            "metadata": {
                "azdata_cell_guid": "764626e4-7c33-478e-8919-664f728a5fe6",
                "tags": []
            },
            "outputs": [],
            "execution_count": 7
        },
        {
            "cell_type": "code",
            "source": [
                "# Variáveis de data e hora\r\n",
                "ref_data = date.today()\r\n",
                "ref_year = date.today().year\r\n",
                "ref_time = datetime.now().strftime(\"%H%M%S\")"
            ],
            "metadata": {
                "azdata_cell_guid": "e9c3c2ae-e3a6-4d1d-96b9-d218ae9d23e3"
            },
            "outputs": [],
            "execution_count": 8
        },
        {
            "cell_type": "code",
            "source": [
                "# Parametros para a criação da string de conexão ao banco sql\r\n",
                "server = 'localhost'\r\n",
                "database = 'db_datasus'\r\n",
                "username = 'sa'\r\n",
                "password = 'SenhaDev1234'\r\n",
                "\r\n",
                "# String de conexão ao banco sql\r\n",
                "conn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+password)\r\n",
                "\r\n",
                "query = \"\"\"SELECT [date] \r\n",
                "                ,[state] \r\n",
                "                ,SUM([confirmed]) AS [confirmed] \r\n",
                "                ,SUM([deaths]) AS [deaths] \r\n",
                "                ,[estimated_population_2019] \r\n",
                "                ,[estimated_population] \r\n",
                "                ,[city_ibge_code] \r\n",
                "                ,[confirmed_per_100k_inhabitants] \r\n",
                "                ,[death_rate] \r\n",
                "            FROM [db_datasus].[dbo].[COVID] \r\n",
                "            WHERE city_ibge_code < 100 \r\n",
                "            GROUP BY  \r\n",
                "                  [date] \r\n",
                "                  ,[state] \r\n",
                "                  ,[estimated_population_2019] \r\n",
                "                  ,[estimated_population] \r\n",
                "                  ,[city_ibge_code] \r\n",
                "                  ,[confirmed_per_100k_inhabitants] \r\n",
                "                  ,[death_rate] \"\"\"\r\n",
                "\r\n",
                "sql_query = pd.read_sql_query(query, conn)"
            ],
            "metadata": {
                "azdata_cell_guid": "ce68e139-9dbd-454c-aa10-52c42a1431e4"
            },
            "outputs": [],
            "execution_count": 169
        },
        {
            "cell_type": "code",
            "source": [
                "# Cria o dataset pandas a partir da consulta sql\r\n",
                "df = pd.DataFrame(sql_query)"
            ],
            "metadata": {
                "azdata_cell_guid": "a0e010fc-ff48-4bca-a060-322eb1c7324f"
            },
            "outputs": [],
            "execution_count": 170
        },
        {
            "cell_type": "code",
            "source": [
                "# df.to_parquet('df.parquet.snappy',compression='snappy')\r\n",
                "# pd.read_parquet('df.parquet.snappy')"
            ],
            "metadata": {
                "azdata_cell_guid": "f7b65be3-a277-4377-968a-e96e882dd5ed"
            },
            "outputs": [],
            "execution_count": 171
        },
        {
            "cell_type": "code",
            "source": [
                "# Realiza a conversa do dataset em pandas para uma table\r\n",
                "table = pa.Table.from_pandas(df)"
            ],
            "metadata": {
                "azdata_cell_guid": "765f42a6-feaa-4f8f-b41f-e44f0770103b",
                "tags": []
            },
            "outputs": [],
            "execution_count": 172
        },
        {
            "cell_type": "code",
            "source": [
                "# Declara as variáveis de dia e hora corrente para montar o nome do arquivo\r\n",
                "date = datetime.now().strftime('%Y%m%d')\r\n",
                "time = str(ref_time)\r\n",
                "\r\n",
                "# Pasta e nome do arquivo no diretório temporário\r\n",
                "filename = '/app/hadoop/tmp/dfs/tmp/etl_hdfs'+'_'+date +'_'+ time\r\n",
                "\r\n",
                "# # Convert a table para o arquivo .parquet\r\n",
                "pq.write_table(table, filename+'.parquet')"
            ],
            "metadata": {
                "azdata_cell_guid": "12e4ce2b-1a90-49b0-a6bc-abc02ce4d213",
                "tags": []
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "'/app/hadoop/tmp/dfs/tmp/etl_hdfs_20210429_153019'"
                    },
                    "metadata": {},
                    "execution_count": 10,
                    "output_type": "execute_result"
                }
            ],
            "execution_count": 10
        },
        {
            "cell_type": "code",
            "source": [
                "# Criação da função para realizando do PUT no HDFS\n",
                "def run_cmd(args_list):\n",
                "    \"\"\"\n",
                "    run linux commands\n",
                "    \"\"\"\n",
                "    # import subprocess\n",
                "    print('Running system command: {0}'.format(' '.join(args_list)))\n",
                "    proc = subprocess.Popen(args_list, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
                "    s_output, s_err = proc.communicate()\n",
                "    s_return = proc.returncode\n",
                "    return s_return, s_output, s_err"
            ],
            "metadata": {
                "azdata_cell_guid": "5ae7a00e-3644-4d34-babe-27d67d7c7c0f"
            },
            "outputs": [],
            "execution_count": 174
        },
        {
            "cell_type": "code",
            "source": [
                "# Variáveis para criação do Caminho de Destino no HDFS\r\n",
                "dir_ano = str(ref_year)\r\n",
                "dir_mes = datetime.now().strftime('%m')\r\n",
                "dir_dia = datetime.now().strftime('%d')\r\n",
                "\r\n",
                "# Caminho de origem do arquivo .parquet\r\n",
                "source_dir = '/app/hadoop/tmp/dfs/tmp/'\r\n",
                "sources = glob.glob(os.path.join(source_dir,\"*.parquet\"))\r\n",
                "\r\n",
                "# Caminho de destino do arquivo .parquet\r\n",
                "(ret, out, err) = run_cmd(['hadoop', 'fs', '-put', sources[0], '/user/datalake/raw/'+str(dir_ano)+'/'+str(dir_mes)+'/'+str(dir_dia)])"
            ],
            "metadata": {
                "azdata_cell_guid": "d8e8b61a-48cf-4c9a-b764-9f6bfcd29e8b",
                "tags": []
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Running system command: hadoop fs -put /app/hadoop/tmp/dfs/tmp/etl_hdfs_2021-04-29_150832.parquet /user/datalake/raw/2021/04/29\n"
                }
            ],
            "execution_count": 175
        },
        {
            "cell_type": "code",
            "source": [
                "# Rotina para limpar a pasta Temp após o input no HDFS\n",
                "source_dir = '/app/hadoop/tmp/dfs/tmp/'\n",
                "sources = glob.glob(os.path.join(source_dir,\"*.parquet\"))\n",
                "\n",
                "for f in sources:\n",
                "    os.remove(f)"
            ],
            "metadata": {
                "azdata_cell_guid": "da6de718-b6bb-493e-a815-f78649556d28",
                "tags": []
            },
            "outputs": [],
            "execution_count": 176
        }
    ]
}