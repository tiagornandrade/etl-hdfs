{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.6.6",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "code",
            "source": [
                "import pandas as pd\r\n",
                "import pyodbc\r\n",
                "import pyarrow as pa\r\n",
                "import pyarrow.parquet as pq\r\n",
                "import os\r\n",
                "import subprocess\r\n",
                "import shutil\r\n",
                "import glob\r\n",
                "from datetime import date, datetime\r\n",
                "from time import time\r\n",
                "from subprocess import Popen, PIPE\r\n",
                "\r\n",
                "print('Importação das bibliotecas', '                 -------- OK')"
            ],
            "metadata": {
                "azdata_cell_guid": "764626e4-7c33-478e-8919-664f728a5fe6",
                "tags": []
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Importação das bibliotecas                  -------- OK\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 37
        },
        {
            "cell_type": "code",
            "source": [
                "# Variáveis de data e hora\r\n",
                "ref_data = date.today()\r\n",
                "ref_year = date.today().year\r\n",
                "ref_time = datetime.now().strftime(\"%H%M%S\")\r\n",
                "\r\n",
                "print('Definição das variáveis de tempo', '                 -------- OK')"
            ],
            "metadata": {
                "azdata_cell_guid": "e9c3c2ae-e3a6-4d1d-96b9-d218ae9d23e3"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Definição das variáveis de tempo",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "                  -------- OK\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 38
        },
        {
            "cell_type": "code",
            "source": [
                "# Parametros para a criação da string de conexão ao banco sql\r\n",
                "server = 'localhost'\r\n",
                "database = 'db_datasus'\r\n",
                "username = 'sa'\r\n",
                "password = 'Ribeiro83'\r\n",
                "\r\n",
                "# String de conexão ao banco sql\r\n",
                "conn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+password)\r\n",
                "\r\n",
                "query = \"\"\"SELECT [date] \r\n",
                "                ,[state] \r\n",
                "                ,SUM([confirmed]) AS [confirmed] \r\n",
                "                ,SUM([deaths]) AS [deaths] \r\n",
                "                ,[estimated_population_2019] \r\n",
                "                ,[estimated_population] \r\n",
                "                ,[city_ibge_code] \r\n",
                "                ,[confirmed_per_100k_inhabitants] \r\n",
                "                ,[death_rate] \r\n",
                "            FROM [db_datasus].[dbo].[COVID] \r\n",
                "            WHERE city_ibge_code < 100 \r\n",
                "            GROUP BY  \r\n",
                "                  [date] \r\n",
                "                  ,[state] \r\n",
                "                  ,[estimated_population_2019] \r\n",
                "                  ,[estimated_population] \r\n",
                "                  ,[city_ibge_code] \r\n",
                "                  ,[confirmed_per_100k_inhabitants] \r\n",
                "                  ,[death_rate] \"\"\"\r\n",
                "\r\n",
                "sql_query = pd.read_sql_query(query, conn)\r\n",
                "\r\n",
                "print('Consulta sql no banco', '                 -------- OK')"
            ],
            "metadata": {
                "azdata_cell_guid": "ce68e139-9dbd-454c-aa10-52c42a1431e4"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Consulta sql no banco                  -------- OK\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 39
        },
        {
            "cell_type": "code",
            "source": [
                "# Cria o dataset pandas a partir da consulta sql\r\n",
                "df = pd.DataFrame(sql_query)\r\n",
                "\r\n",
                "print('Criação do dataset pandas', '                 -------- OK')"
            ],
            "metadata": {
                "azdata_cell_guid": "a0e010fc-ff48-4bca-a060-322eb1c7324f",
                "tags": []
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Criação do dataset pandas                  -------- OK\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 40
        },
        {
            "cell_type": "code",
            "source": [
                "# Realiza a conversa do dataset em pandas para uma table\r\n",
                "table = pa.Table.from_pandas(df)\r\n",
                "\r\n",
                "print('Conversão do dataset pandas em table parquet', '                 -------- OK')"
            ],
            "metadata": {
                "azdata_cell_guid": "765f42a6-feaa-4f8f-b41f-e44f0770103b",
                "tags": []
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Conversão do dataset pandas em table parquet",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "                  -------- OK\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 41
        },
        {
            "cell_type": "code",
            "source": [
                "# Declara as variáveis de dia e hora corrente para montar o nome do arquivo\r\n",
                "date = datetime.now().strftime('%Y%m%d')\r\n",
                "time = str(ref_time)\r\n",
                "\r\n",
                "# Pasta e nome do arquivo no diretório temporário\r\n",
                "filename = 'C:/Temp/stage/etl_hdfs'+'_'+date +'_'+ time\r\n",
                "\r\n",
                "# Convert a table para o arquivo .parquet\r\n",
                "pq.write_table(table, filename+'.parquet')\r\n",
                "\r\n",
                "print('Conversão da table em arquivo parquet', '                 -------- OK')"
            ],
            "metadata": {
                "azdata_cell_guid": "ad982a48-eb02-4501-b63d-6e7bfa874d7d"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Conversão da table em arquivo parquet                  -------- OK\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 42
        },
        {
            "cell_type": "code",
            "source": [
                "# Variáveis para criação do Caminho de Destino no HDFS\r\n",
                "dir_ano = str(ref_year)\r\n",
                "dir_mes = datetime.now().strftime('%m')\r\n",
                "dir_dia = datetime.now().strftime('%d')\r\n",
                "\r\n",
                "# Criação da pasta raw no hdfs\r\n",
                "raw = 'hadoop fs -mkdir /user/datalake/raw'\r\n",
                "os.system(raw)\r\n",
                "print('Criação da pasta: ', '/user/datalake/raw', '                 -------- OK')\r\n",
                "\r\n",
                "# Criação da pasta ano no hdfs\r\n",
                "ano = 'hadoop fs -mkdir /user/datalake/raw/'+str(dir_ano)\r\n",
                "os.system(ano)\r\n",
                "print('Criação da pasta: ', '/user/datalake/raw/'+str(dir_ano), '            -------- OK')\r\n",
                "\r\n",
                "# Criação da pasta ano no hdfs\r\n",
                "mes = 'hadoop fs -mkdir /user/datalake/raw/'+str(dir_ano)+'/'+str(dir_mes)\r\n",
                "os.system(mes)\r\n",
                "print('Criação da pasta: ', '/user/datalake/raw/'+str(dir_ano)+'/'+str(dir_mes), '         -------- OK')\r\n",
                "\r\n",
                "# Criação da pasta ano no hdfs\r\n",
                "dia = 'hadoop fs -mkdir /user/datalake/raw/'+str(dir_ano)+'/'+str(dir_mes)+'/'+str(dir_dia)\r\n",
                "os.system(dia)\r\n",
                "print('Criação da pasta: ', '/user/datalake/raw/'+str(dir_ano)+'/'+str(dir_mes)+'/'+str(dir_dia), '      -------- OK')"
            ],
            "metadata": {
                "azdata_cell_guid": "12e4ce2b-1a90-49b0-a6bc-abc02ce4d213"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Criação da pasta:  /user/datalake/raw                  -------- OK\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Criação da pasta:  /user/datalake/raw/2021             -------- OK\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Criação da pasta:  /user/datalake/raw/2021/05          -------- OK\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Criação da pasta:  /user/datalake/raw/2021/05/03       -------- OK\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 43
        },
        {
            "cell_type": "code",
            "source": [
                "# Variáveis para criação do Caminho de Destino no HDFS\r\n",
                "dir_ano = str(ref_year)\r\n",
                "dir_mes = datetime.now().strftime('%m')\r\n",
                "dir_dia = datetime.now().strftime('%d')\r\n",
                "\r\n",
                "# Caminho de origem do arquivo .parquet\r\n",
                "source_dir = 'C:\\Temp\\stage'\r\n",
                "sources = glob.glob(os.path.join(source_dir,\"*.parquet\"))\r\n",
                "\r\n",
                "# Criação do comando shell para realização do put\r\n",
                "cmd = 'hadoop'+' fs'+' -put '+sources[0]+' /user/datalake/raw/'+str(dir_ano)+'/'+str(dir_mes)+'/'+str(dir_dia)\r\n",
                "\r\n",
                "# Comando put no hdfs\r\n",
                "os.system(cmd)\r\n",
                "\r\n",
                "print('Realização do PUT no HDFS', '                 -------- OK')"
            ],
            "metadata": {
                "azdata_cell_guid": "d8e8b61a-48cf-4c9a-b764-9f6bfcd29e8b",
                "tags": []
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Realização do PUT no HDFS                  -------- OK\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 44
        },
        {
            "cell_type": "code",
            "source": [
                "# Rotina para limpar a pasta Temp após o input no HDFS\r\n",
                "source_dir = 'C:\\Temp\\stage'\r\n",
                "sources = glob.glob(os.path.join(source_dir,\"*.parquet\"))\r\n",
                "\r\n",
                "for f in sources:\r\n",
                "    os.remove(f)\r\n",
                "\r\n",
                "print('Limpeza da pasta temporária', '      -------- OK')"
            ],
            "metadata": {
                "azdata_cell_guid": "8b8d1f00-65a7-4794-b8c1-02103046e83b"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Limpeza da pasta temporária       -------- OK\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 45
        }
    ]
}